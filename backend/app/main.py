from fastapi import FastAPI, Request, HTTPException
from pydantic import BaseModel
from app.services.agent_service import carregar_agente
import time
from datetime import datetime
from fastapi.middleware.cors import CORSMiddleware
from app.utils.memory_pool import get_agent_for_user
from app.core.logger import log_interaction, get_user_history

app = FastAPI()
agente = carregar_agente()

# Adicionando o middleware CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Ou especifique ['http://localhost:5173']
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class Pergunta(BaseModel):
    pergunta: str

@app.get("/")
def index():
    return {"message": "Agente de IA Online"}

@app.post("/user/{user_id}/perguntar")
async def perguntar(user_id: str, request: Request):
    try:
        data = await request.json()
        pergunta = data.get("pergunta", "")
        if not pergunta:
            raise HTTPException(status_code=400, detail="Pergunta n√£o fornecida.")

        agente = get_agent_for_user(user_id)
        # Buscar hist√≥rico recente do usu√°rio
        historico = get_user_history(user_id, limit=5)
        # Montar contexto para a LLM
        contexto = "\n".join([
            f"Usu√°rio: {h['pergunta']}\nAgente: {h['resposta']}" for h in historico
        ])
        # Prompt humanizado e simp√°tico
        instrucao = (
            "Voc√™ √© um assistente virtual da TechStore, sempre simp√°tico, acolhedor e prestativo. "
            "Responda de forma clara, amig√°vel e personalizada, usando linguagem natural, emojis e frases positivas. "
            "Quando explicar produtos, organize as informa√ß√µes em t√≥picos ou listas, use t√≠tulos e destaque pontos importantes. "
            "Seja breve quando poss√≠vel, mas sempre cordial. Se n√£o souber algo, incentive o usu√°rio a perguntar de outra forma ou ofere√ßa ajuda adicional. "
            "Exemplo de sauda√ß√£o: 'Ol√°! üòä Como posso ajudar voc√™ hoje?'\n"
        )
        prompt = f"{contexto}\nUsu√°rio: {pergunta}\nAgente: {instrucao}"
        inicio_execucao = time.time()
        resultado = agente({"query": prompt})
        tempo_exec = f"{(time.time() - inicio_execucao) * 1000:.2f}ms"

        # Serializa os documentos fonte em um formato JSON v√°lido
        fontes_usadas = []
        if resultado.get("source_documents"):
            for doc in resultado["source_documents"]:
                fonte = {
                    "arquivo": str(doc.metadata.get("source", "desconhecido")),
                    "conteudo_relevante": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                }
                fontes_usadas.append(fonte)

        # Garantir que tudo seja serializ√°vel
        response_data = {
            "consulta": {
                "pergunta": str(pergunta),
                "timestamp": datetime.now().isoformat(),
            },
            "resposta": {
                "conteudo": str(resultado.get("result", "N√£o foi poss√≠vel gerar uma resposta.")),
                "fontes_usadas": fontes_usadas
            },
            "metadata": {
                "tempo_execucao": str(tempo_exec),
                "status": "success"
            }
        }

        # Log ap√≥s serializa√ß√£o
        log_interaction(user_id, pergunta, {"result": str(resultado.get("result")), "sources": fontes_usadas})

        return response_data

    except Exception as e:
        error_response = {
            "consulta": {
                "pergunta": str(pergunta),
                "timestamp": datetime.now().isoformat(),
            },
            "resposta": {
                "conteudo": f"Erro ao processar pergunta: {str(e)}",
                "fontes_usadas": []
            },
            "metadata": {
                "tempo_execucao": "0ms",
                "status": "error",
                "erro": str(e)
            }
        }
        return error_response
